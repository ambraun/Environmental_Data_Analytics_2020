---
title: "6: Data Wrangling"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2020"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## Objectives
1. Describe the usefulness of data wrangling and its place in the data pipeline
2. Wrangle datasets with dplyr functions
3. Apply data wrangling skills to a real-world example dataset

## Set up your session

Today we will work with a dataset from the [North Temperate Lakes Long-Term Ecological Research Station](https://lter.limnology.wisc.edu/about/overview). The NTL-LTER is located in the boreal zone in northern Wisconsin, USA. We will use the [chemical and physical limnology dataset](https://lter.limnology.wisc.edu/content/cascade-project-north-temperate-lakes-lter-core-data-physical-and-chemical-limnology-1984), running from 1984-2016. 

Opening discussion: why might we be interested in long-term observations of temperature, oxygen, and light in lakes?

> Add notes here: ecosystem health - eutrophication, 

```{r, message = FALSE}
getwd()
library(tidyverse)

#install.packages('lubridate')

library(lubridate)

#library function should not be commented out, check mark in packages window when it has been loaded 

NTL.phys.data <- read.csv("./Data/Raw/NTL-LTER_Lake_ChemistryPhysics_Raw.csv")

#North Temperature Lakes, physical data, make an object from the CSV

colnames(NTL.phys.data)
head(NTL.phys.data) #first 6 rows, what types of data are in there?
summary(NTL.phys.data)
str(NTL.phys.data) #structure over summary, structure tells you what type of data (vector) is in each column 
dim(NTL.phys.data) #38614 rows, 11 columns

class(NTL.phys.data$sampledate)
# Format sampledate as date
NTL.phys.data$sampledate <- as.Date(NTL.phys.data$sampledate, format = "%m/%d/%y")
class(NTL.phys.data$sampledate)

#formatting it how it is 
```

## Data Wrangling

Data wrangling extends data exploration: it allows you to process data in ways that are useful for you. An important part of data wrangling is creating *tidy datasets*, with the following rules: 

1. Each variable has its own column, can't have multiple items within each cell 
2. Each observation has its own row
3. Each value has its own cell

What is the best way to wrangle data? There are multiple ways to arrive at a specific outcome in R, and we will illustrate some of those approaches. Your goal should be to write the simplest code that will get you to your desired outcome. However, there is sometimes a trade-off of the opportunity cost to learn a new formulation of code and the time it takes to write complex code that you already know. Remember that the best code is one that is easy to understand for yourself and your collaborators. Remember to comment your code, use informative names for variables and functions, and use reproducible methods to arrive at your output.

## Dplyr Wrangling Functions

`dplyr` is a package in R that includes functions for data manipulation (i.e., data wrangling or data munging). `dplyr` is included in the tidyverse package, so you should already have it installed on your machine. The functions act as verbs for data wrangling processes. For more information, run this line of code. Vignette comes up in Help Window. Tidy verse functions have cheat sheet. results = hide, makes it not appear when you knit 

```{r, results = "hide"}
### results = "hide" makes it does vignette doesn't appear in final knit PDF document 
vignette("dplyr")
### document that goes through main function and examples of how to use each function 
### a lot of tidyvers functions also have cheat sheets 
```

### Filter

Filtering allows us to choose certain rows (observations) in our dataset. Need to know which type of data are in each 

Here are the relevant commands used in the `filter` function. Add some notes to designate what these commands mean. 
`==` equals, factor level or numeric level, make sure to use two with filter function 
`!=` not equal to 
`<` less than 
`<=` less than or equal 
`>` greater than
`>=` greater than or equal 
`&` and 
`|` or 

```{r}
class(NTL.phys.data$lakeid)
# factor vector, letter in each cell 
class(NTL.phys.data$depth)
#depth in meters, numeric vector 

# matrix filtering
NTL.phys.data.surface1 <- NTL.phys.data[NTL.phys.data$depth == 0,]
### just surface depths (zero)
### in matrix notation, must include comma so that it indicates every column 

# dplyr filtering
NTL.phys.data.surface2 <- filter(NTL.phys.data, depth == 0)
NTL.phys.data.surface3 <- filter(NTL.phys.data, depth < 0.25)
# what the data are and how to do the filtering, grab all phys data, find rows where depth =0

# Did the methods arrive at the same result?
head(NTL.phys.data.surface1)
dim(NTL.phys.data.surface1)
head(NTL.phys.data.surface2)
dim(NTL.phys.data.surface2)
head(NTL.phys.data.surface3)
dim(NTL.phys.data.surface3)

# Choose multiple conditions to filter
summary(NTL.phys.data$lakename)
NTL.phys.data.PeterPaul1 <- filter(NTL.phys.data, lakename == "Paul Lake" | lakename == "Peter Lake")
#create data frame that lake name = Paul Lake OR lakename = Peter Lake 
NTL.phys.data.PeterPaul2 <- filter(NTL.phys.data, lakename != "Central Long Lake" & 
                                     lakename != "Crampton Lake" & lakename != "East Long Lake" &
                                     lakename != "Hummingbird Lake" & lakename != "Tuesday Lake" &
                                     lakename != "Ward Lake" & lakename != "West Long Lake")
#create data frame that lake name is NOT EQUAL to these lake names 
NTL.phys.data.PeterPaul3 <- filter(NTL.phys.data, lakename %in% c("Paul Lake", "Peter Lake"))
#%in% include anything that is Paul or Peter Lake, concatanate that list, within filter only concatanate when using the %in% function 

# Choose a range of conditions of a numeric or integer variable/value
#day of the year when each sample was taken 
summary(NTL.phys.data$daynum)
NTL.phys.data.JunethruOctober1 <- filter(NTL.phys.data, daynum > 151 & daynum < 305)
#filtering to select days greater than day 151 and less than day 305, & operator 
NTL.phys.data.JunethruOctober2 <- filter(NTL.phys.data, daynum > 151, daynum < 305)
#same but with comma operator 
NTL.phys.data.JunethruOctober3 <- filter(NTL.phys.data, daynum >= 152 & daynum <= 304)
#another syntax 
NTL.phys.data.JunethruOctober4 <- filter(NTL.phys.data, daynum %in% c(152:304))
#includes operator with a cancatanated range of numbers 

# Exercise: 
# filter NTL.phys.data for the year 1999
# what code do you need to use, based on the class of the variable?
class(NTL.phys.data$year4)
NTL.phys.data.1999 <- filter(NTL.phys.data, year4 == 1999)
# Exercise: 
# filter NTL.phys.data for Tuesday Lake from 1990 through 1999.
NTL.phys.data.Tuesday <-filter(NTL.phys.data, 
                               lakename == "Tuesday Lake" & 
                                 year4 > 1989 & year4 <2000)
# or year %in% c(1990:1999)
dim(NTL.phys.data.Tuesday)
dim(NTL.phys.data.1999)
```
Question: Why don't we filter using row numbers?

> Answer: Wanted to filter based on attribute than row numbers, if the data set changes, the rows wouldn't be the right ones. 

### Arrange

Arranging allows us to change the order of rows in our dataset. By default, the arrange function will arrange rows in ascending order.

```{r}
NTL.phys.data.depth.ascending <- arrange(NTL.phys.data, depth)
#arrange physical data set by ascending depth, dont have to specify ascending  
NTL.phys.data.depth.descending <- arrange(NTL.phys.data, desc(depth))
#arrange physical data set by descending order, do have to specify descending 

# Exercise: 
# Arrange NTL.phys.data by temperature, in descending order. 
NTL.phys.data.temperature.descending <- arrange(NTL.phys.data, desc(temperature_C))
# Which dates, lakes, and depths have the highest temperatures?
#midsummer, shallow depths, june and july, some lakes more than others 

```
### Select

Selecting allows us to choose certain columns (variables) in our dataset.

```{r}
NTL.phys.data.temps <- select(NTL.phys.data, lakename, sampledate:temperature_C)
#name which column want to keep or leave out of data set, select columns lakename and include sample date through temperature_C, selecting only based on name of the column, select needs commas for what you're selecting, can also put that you don't want to include lake name with a -lakename, would have to be both -- exclude, or both include 

```
### Mutate

Mutating allows us to add new columns that are functions of existing columns. Operations include addition, subtraction, multiplication, division, log, and other functions.

```{r}

NTL.phys.data.temps <- mutate(NTL.phys.data.temps, temperature_F = (temperature_C*9/5) + 32)
#manipuilate from F to C, will produce a new column, always mutates onto the end of the data  (call up data frame, name the new column = what you want it to equal)

```

## Lubridate

A package that makes coercing date much easier is `lubridate`. A guide to the package can be found at https://lubridate.tidyverse.org/. The cheat sheet within that web page is excellent too. This package can do many things (hint: look into this package if you are having unique date-type issues), but today we will be using two of its functions for our NTL dataset. 

```{r}

# Lubridate has nice help section for the package 
# add a month column to the dataset
NTL.phys.data.PeterPaul1 <- mutate(NTL.phys.data.PeterPaul1, month = month(sampledate)) 
#create a new column called month, whatever lubridate works on has to be a date already, puts a month column at the end 

# reorder columns to put month with the rest of the date variables
NTL.phys.data.PeterPaul1 <- select(NTL.phys.data.PeterPaul1, lakeid:daynum, month, sampledate:comments)
#rearranging order of columns: lake id through daynumber, then month, then sample date through comments 

view<- NTL.phys.data.PeterPaul1
# find out the start and end dates of the dataset
interval(NTL.phys.data.PeterPaul1$sampledate[1], NTL.phys.data.PeterPaul1$sampledate[21613])
interval(first(NTL.phys.data.PeterPaul1$sampledate), last(NTL.phys.data.PeterPaul1$sampledate))
#find out the first and last sample dates 

```


## Pipes

Sometimes we will want to perform multiple functions on a single dataset on our way to creating a processed dataset. We could do this in a series of subsequent functions or create a custom function. However, there is another method to do this that looks cleaner and is easier to read. This method is called a pipe. We designate a pipe with `%>%`. A good way to think about the function of a pipe is with the word "then." 

Let's say we want to take our raw dataset (NTL.phys.data), *then* filter the data for Peter and Paul lakes, *then* select temperature and observation information, and *then* add a column for temperature in Fahrenheit: 

```{r}
#pipe operator %operator% - pulls out the name of the data frame and uses the title of it in the first , the pipe means "then" , take the physical data set, "then" do something, "then" do something else, 
NTL.phys.data.processed <- 
  NTL.phys.data %>%
  filter(lakename == "Paul Lake" | lakename == "Peter Lake") %>%
  select(lakename, sampledate:temperature_C) %>%
  mutate(temperature_F = (temperature_C*9/5) + 32)
  
```

Notice that we did not place the dataset name inside the wrangling function but rather at the beginning.

### Saving processed datasets

```{r}
#row names = false, doesn't produce a row of numbers at the beginning 
write.csv(NTL.phys.data.PeterPaul1, row.names = FALSE, file = "./Data/Processed/NTL-LTER_Lake_ChemistryPhysics_PeterPaul_Processed.csv")
```

## Closing Discussion

When we wrangle a raw dataset into a processed dataset, we create a code file that contains only the wrangling code. We then save the processed dataset as a new spreadsheet and then create a separate code file to analyze and visualize the dataset. Why do we keep the wrangling code separate from the analysis code?


