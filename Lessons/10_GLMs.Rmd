---
title: "10: Generalized Linear Models (T-Test)"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2020"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## Objectives
1. Describe the components of the generalized linear model (GLM)
2. Apply special cases of the GLM (t-test) to real datasets
3. Interpret and report the results of t-tests in publication-style formats

## Set up
```{r, message = FALSE}
getwd()
library(tidyverse)

EPAair <- read.csv("./Data/Processed/EPAair_O3_PM25_NC1819_Processed.csv")

# Set date to date format
EPAair$Date <- as.Date(EPAair$Date, format = "%Y-%m-%d")

# Set theme
mytheme <- theme_classic(base_size = 14) +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "top")
theme_set(mytheme)
```

## Generalized Linear Models (GLMs)

The one-sample test (model of the mean), two-sample t-test, analysis of variance (ANOVA), and linear regression are all special cases of the **generalized linear model** (GLM). The GLM also includes analyses not covered in this class, including logistic regression, multinomial regression, chi square, and log-linear models. The common characteristic of general linear models is the expression of a continuous response variable as a linear combination of the effects of categorical or continuous explanatory variables, plus an error term that expresses the random error associated with the coefficients of all explanatory variables. The explanatory variables comprise the deterministic component of the model, and the error term comprises the stochastic component of the model. Historically, artificial distinctions were made between linear models that contained categorical and continuous explanatory variables, but this distinction is no longer made. The inclusion of these models within the umbrella of the GLM allows models to fit the main effects of both categorical and continuous explanatory variables as well as their interactions. 

### Choosing a model from your data: A "cheat sheet"

Continuous Variable. If a variable can take on any value between its minimum value and its maximum value, it is called a continuous variable; otherwise, it is called a discrete variable. ... Therefore, the number of heads must be a discrete variable.

**T-test:** Continuous response, one categorical explanatory variable with two categories (or comparison to a single value if a one-sample test).
One-Sample t-Test. A one-sample t-test is used to test whether a population mean is significantly different from some hypothesized value.
A two-sample t-test is used to test the difference between two population means. A common application is to determine whether the means are equal.

ANOVA stands for analysis of variance and tests for differences in the effects of independent variables on a dependent variable.
**One-way ANOVA (Analysis of Variance):** Continuous response, one categorical explanatory variable with more than two categories.
The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of two or more independent (unrelated) groups.

**Two-way ANOVA (Analysis of Variance)** Continuous response, two categorical explanatory variables.
A two-way ANOVA test is a statistical test used to determine the effect of two nominal predictor variables on a continuous outcome variable. A two-way ANOVA tests the effect of two independent variables on a dependent variable. A two-way ANOVA test analyzes the effect of the independent variables on the expected outcome along with their relationship to the outcome itself.

**Single Linear Regression** Continuous response, one continuous explanatory variable.
Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables: ... The other variable, denoted y, is regarded as the response, outcome, or dependent variable. A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable.

**Multiple Linear Regression** Continuous response, two or more continuous explanatory variables.Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).

**ANCOVA (Analysis of Covariance)** Continuous response, categorical explanatory variable(s) and  continuous explanatory variable(s).

If multiple explanatory variables are chosen, they may be analyzed with respect to their **main effects** on the model (i.e., their separate impacts on the variance explained) or with respsect to their **interaction effects,** the effect of interacting explanatory variables on the model. 

### Assumptions of the GLM

The GLM is based on the assumption that the data residuals approximate a normal distribution (or a linearly transformed normal distribution). We will discuss the non-parametric analogues to several of these tests if the assumptions of normality are violated. For tests that analyze categorical explanatory variables, the assumption is that the variance in the response variable is equal among groups. Note: environmental data often violate the assumptions of normality and equal variance, and we will often proceed with a GLM even if these assumptions are violated. In this situation, justifying the decision to proceed with a linear model must be made. 

Nonparametric statistics refer to a statistical method in which the data is not required to fit a normal distribution. Nonparametric statistics uses data that is often ordinal, meaning it does not rely on numbers, but rather on a ranking or order of sorts.

## T-Test
### One-sample t-test
The object of a one sample test is to test the null hypothesis that the mean of the group is equal to a specific value. For example, we might ask ourselves (from the EPA air quality processed dataset): 

In statistics, the p-value is the probability of obtaining the observed results of a test, assuming that the null hypothesis is correct. ... A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis

In statistical hypothesis testing, the p-value or probability value is the probability of obtaining test results at least as extreme as the results actually observed during the test, assuming that the null hypothesis is correct

The Shapiro-Wilk test is a way to tell if a random sample comes from a normal distribution. The test gives you a W value; small values indicate your sample is not normally distributed (you can reject the null hypothesis that your population is normally distributed if your values are under a certain threshold).

Are Ozone levels below the threshold for "good" AQI index (0-50)?

```{r}
# test null hypothesis of normal distribution 

summary(EPAair$Ozone)
# summary = distrubtion 

EPAair.subsample <- sample_n(EPAair, 5000)

### why don't we just use the total number of observations?

# Evaluate assumption of normal distribution
shapiro.test((EPAair.subsample$Ozone))
ggplot(EPAair, aes(x = Ozone)) +
  geom_histogram() 
qqnorm(EPAair$Ozone); qqline(EPAair$Ozone)

### what is the W value, and how do we know what to compare it to?, 

##qqline shows what a normal distribution would look like 
##verify that our data is not fit on a normal plot 

O3.onesample <- t.test(EPAair$Ozone, mu = 50, alternative = "less")
O3.onesample

###what is the t-statistic and what does it mean? what are degrees of freedom? what is considerd a small pvalue?

#t.test runs t test, give it one column in data frame, comparing a mu (mean) of 50,   alternative hypothesis is that mean is less than 50 

Ozone.plot <- ggplot(EPAair, aes(x = Ozone)) +
  geom_density(stat = "count", fill = "gray") +
  #geom_density(fill = "gray") +
  geom_vline(xintercept = 50, color = "#238b45", lty = 2, size = 0.9) +
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))
print(Ozone.plot)

#geom vline - what you're test against, line type of 2 gives dashed line, size adjusts thickness of line 
#last line of code specifies using origin for the starting point 
#majority of samples are less than 50 

#better to include this type of plot in a report, not a test statistic 

#geomdensity (state = "count") shows the actual count, based on how thick you want the bins to be 

###geomdensity(fill = "Gray") moving average of the distribution, smoothing across the dataset, what does that mean?


```

Write a sentence or two about the results of this test. Include both the results of the test and an interpretation that puts the findings in context of the resarch question.

> EPA ozone measurements for 2018-2019 were significantly lower than 50, the AQI threshold for "good" air quality (one-sample t-test; t = -57.98, p <.0001). 
not putting in alternative or null hypothesis, don't put t-test statistic and p- value statistic in the main sentence. Careful to use the word significantly when using statistical results. 

### Two-sample t-test
The two-sample *t* test is used to test the hypothesis that the mean of two samples is equivalent. Unlike the one-sample tests, a two-sample test requires a second assumption that the variance of the two groups is equivalent. Are Ozone levels different between 2018 and 2019? Testing means of two different groups, variance of groups relatively equal? 

```{r}
shapiro.test(EPAair$Ozone[EPAair$Year == 2018])
shapiro.test(EPAair$Ozone[EPAair$Year == 2019])
var.test(EPAair$Ozone ~ EPAair$Year)

###I dont understand this syntax of this line 
#subset square brackts where year = 2018, example of matrix subsetting 
###not normal distribution because p values are very low - is this true?
# tilde ~ means "BY" 
#var.test gives ratio of variances, variances are significantly different from each other 
#violated the assumption of equal variance 

ggplot(EPAair, aes(x = Ozone, color = as.factor(Year))) +
  geom_freqpoly()

#as.factor = year is listed as character or numeric, as.factor counts year as a factor instead of a continuous variable 
#the color is years from the year column as a factor 

# Format as a t-test
O3.twosample <- t.test(EPAair$Ozone ~ EPAair$Year)
O3.twosample
O3.twosample$p.value
#instead of comparing to a mu, comparing Ozone by Year, we're not testing an alternative hypothesis 
#Welch Two Sample T-Test Data produced, df = degrees of freedom, metric of effective sample size 
# last line $p-value - can grab a specific element from a test, instead of the whole output of the t.test 

# Format as a GLM
O3.twosample2 <- lm(EPAair$Ozone ~ EPAair$Year)
summary(O3.twosample2)

#linear model, summary of the object 
#format the same, interpret slightly differently 

plot(O3.twosample2)
par(mfrow = c(2,2))
#c 2 rows, 2 columns, changes the way R displays your plot 

###what does that last line mean?? what does any of this mean?
###what are standardized residuals?
##The Standardized Residual is defined as the Residual divided by its standard deviation, where the residual is the difference between the data response and the fitted response.
```

### Non-parametric equivalent of t-test: Wilcoxon test

When we wish to avoid the assumption of normality, we can apply *distribution-free*, or non-parametric, methods in the form of the Wilcoxon rank sum (Mann-Whitney) test. The Wilcoxon test replaces the data by their rank and calculates the sum of the ranks for each group. Notice that the output of the Wilcoxon test is more limited than its parametric equivalent. # Not assuming any specific distribution in our dataset. Ranks data from lowest to highest, computes the rank of each data point. 

```{r}
O3.onesample.wilcox <- wilcox.test(EPAair$Ozone, mu = 50, alternative = "less")
O3.onesample.wilcox
#less statistical robustness compared to above, get whether alternative hypothesis is supported or not 
###what are standardized residuals? What is cook's distance?
#V result

twosample.wilcox <- wilcox.test(EPAair$Ozone ~ EPAair$Year)
# W result 
O3.twosample.wilcox
```

### Visualization and interpretation challenge

Create three plots, each with appropriately formatted axes and legends. Choose a non-default color palette.

1. geom_density of ozone divided by year (distinguish between years by adding transparency to the geom_density layer).
2. geom_boxplot of ozone divided by year . Add letters representing a significant difference between 2018 and 2019 (hint: stat_summary). 
3. geom_violin of ozone divided by year, with the 0.5 quantile marked as a horizontal line. Add letters representing a significant difference between 2018 and 2019. 

```{r}
Ozone.plot2 <- ggplot(EPAair, x = Ozone, y = as.factor(Year)) +
  geom_density(EPAair$Ozone ~ EPAair$Year)
print(Ozone.plot2)
Ozone.plot <- ggplot(EPAair, aes(x = Ozone)) +
  geom_density(stat = "count", fill = "gray")
print(Ozone.plot2)
```

Now, write a summary of your findings, incorporating statistical output, reference to the figure(s), and a contextual interpretation.

> 



